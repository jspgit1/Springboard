{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model tuning\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Modeling\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Splitting data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "N_FOLDS = 5\n",
    "MAX_EVALS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv('application_train.csv')\n",
    "\n",
    "# Sample 16000 rows (10000 for training, 6000 for testing)\n",
    "features = features.sample(n = 16000, random_state = 42)\n",
    "\n",
    "# Only numeric features\n",
    "features = features.select_dtypes('number')\n",
    "\n",
    "# Extract the labels\n",
    "labels = np.array(features['TARGET'].astype(np.int32)).reshape((-1, ))\n",
    "features = features.drop(columns = ['TARGET', 'SK_ID_CURR'])\n",
    "\n",
    "# Split into training and testing data\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 6000, random_state = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training features shape: \", train_features.shape)\n",
    "print(\"Testing features shape: \", test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training and testing dataset\n",
    "train_set = lgb.Dataset(data = train_features, label = train_labels)\n",
    "test_set = lgb.Dataset(data = test_features, label = test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get default hyperparameters\n",
    "model = lgb.LGBMClassifier()\n",
    "default_params = model.get_params()\n",
    "\n",
    "# Remove the number of estimators because we set this to 10000 in the cv call\n",
    "del default_params['n_estimators']\n",
    "\n",
    "# Cross validation with early stopping\n",
    "cv_results = lgb.cv(default_params, train_set, num_boost_round = 10000, early_stopping_rounds = 100, \n",
    "                    metrics = 'auc', nfold = N_FOLDS, seed = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The maximum validation ROC AUC was: {:.5f} with a standard deviation of {:.5f}.'.format(cv_results['auc-mean'][-1], cv_results['auc-stdv'][-1]))\n",
    "print('The optimal number of boosting rounds (estimators) was {}.'.format(len(cv_results['auc-mean'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "# Optimal number of esimators found in cv\n",
    "model.n_estimators = len(cv_results['auc-mean'])\n",
    "\n",
    "# Train and make predicions with model\n",
    "model.fit(train_features, train_labels)\n",
    "preds = model.predict_proba(test_features)[:, 1]\n",
    "baseline_auc = roc_auc_score(test_labels, preds)\n",
    "\n",
    "print('The baseline model scores {:.5f} ROC AUC on the test set.'.format(baseline_auc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(hyperparameters, iteration):\n",
    "    \"\"\"Objective function for grid and random search. Returns\n",
    "       the cross validation score from a set of hyperparameters.\"\"\"\n",
    "    \n",
    "    # Number of estimators will be found using early stopping\n",
    "    if 'n_estimators' in hyperparameters.keys():\n",
    "        del hyperparameters['n_estimators']\n",
    "    \n",
    "     # Perform n_folds cross validation\n",
    "    cv_results = lgb.cv(hyperparameters, train_set, num_boost_round = 10000, nfold = N_FOLDS, \n",
    "                        early_stopping_rounds = 100, metrics = 'auc', seed = 42)\n",
    "    \n",
    "    # results to retun\n",
    "    score = cv_results['auc-mean'][-1]\n",
    "    estimators = len(cv_results['auc-mean'])\n",
    "    hyperparameters['n_estimators'] = estimators \n",
    "    \n",
    "    return [score, hyperparameters, iteration]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, params, iteration = objective(default_params, 1)\n",
    "\n",
    "print('The cross-validation ROC AUC was {:.5f}.'.format(score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a default model\n",
    "model = lgb.LGBMModel()\n",
    "model.get_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'boosting_type': ['gbdt', 'goss', 'dart'],\n",
    "    'num_leaves': list(range(20, 150)),\n",
    "    'learning_rate': list(np.logspace(np.log10(0.005), np.log10(0.5), base = 10, num = 1000)),\n",
    "    'subsample_for_bin': list(range(20000, 300000, 20000)),\n",
    "    'min_child_samples': list(range(20, 500, 5)),\n",
    "    'reg_alpha': list(np.linspace(0, 1)),\n",
    "    'reg_lambda': list(np.linspace(0, 1)),\n",
    "    'colsample_bytree': list(np.linspace(0.6, 1, 10)),\n",
    "    'subsample': list(np.linspace(0.5, 1, 100)),\n",
    "    'is_unbalance': [True, False]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(50)\n",
    "\n",
    "# Randomly sample a boosting type\n",
    "boosting_type = random.sample(param_grid['boosting_type'], 1)[0]\n",
    "\n",
    "# Set subsample depending on boosting type\n",
    "subsample = 1.0 if boosting_type == 'goss' else random.sample(param_grid['subsample'], 1)[0]\n",
    "\n",
    "print('Boosting type: ', boosting_type)\n",
    "print('Subsample ratio: ', subsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Learning rate histogram\n",
    "plt.hist(param_grid['learning_rate'], bins = 20, color = 'r', edgecolor = 'k');\n",
    "plt.xlabel('Learning Rate', size = 14); plt.ylabel('Count', size = 14); plt.title('Learning Rate Distribution', size = 18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "b = 0\n",
    "\n",
    "# Check number of values in each category\n",
    "for x in param_grid['learning_rate']:\n",
    "    # Check values\n",
    "    if x >= 0.005 and x < 0.05:\n",
    "        a += 1\n",
    "    elif x >= 0.05 and x < 0.5:\n",
    "        b += 1\n",
    "\n",
    "print('There are {} values between 0.005 and 0.05'.format(a))\n",
    "print('There are {} values between 0.05 and 0.5'.format(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
